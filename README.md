# Text Not Found ...

Here we present a work analyzing the authorship of books by a very productive writer and private detective enthusiast.  
She has been writing for 26 years and has already published **381 books**.  
This means she writes around **15 books per year** ‚Äî or **1.2 per month**.  
 
<img src="plots/writing_stat.png" alt="Writing trend" width="400">

Suspicious, isn‚Äôt it? Some people even believe that the writer‚Äôs pugs (yes, small dogs!) help her type the books. üêï


<img src="images/pug_detective.jpeg" alt="Pug detective" width="100">

*The real ghostwriters? We may never know...*

So, we decided to analyse the authorship of these books to shed light on this mystery. We also hope this project can inspire similar analyses in other fields.  
Of course, we do not claim **100% truth** in our conclusions ‚Äî but you can join us on this journey to solve the riddle.

A small spoiler: we created a **pipeline for text authorship analysis using k-mers**. This is a new approach for stylometry, so be patient and read until the end!


## Data description

- **341 books** of the author of interest
- **10 other detective novels** (control group 1)
- **10 other detective novels** (control group 2)
- **10 other detective novels** (control group 3)
- **5 generated texts**
- **5 interviews**
- **2 fan-fiction**
- **And 6 pugs** (ok, that last one is a joke ‚Äî actually 6 funny and positive people üòÑ)

## Required packages

1. **[Stylo](https://github.com/computationalstylistics/stylo)** (R package for stylometry)
2. **[spaCy](https://spacy.io/usage/spacy-101#whats-spacy)** (advanced NLP library for Python), used for preprocessing
3. **[GigaChain](https://github.com/ai-forever/gigachain)** used for text generation in the style of the author
4. **[scikit-learn](https://scikit-learn.org/stable/)** used for dimentionality reduction
5. [–ë–∏–±–ª–∏–æ—Ç–µ–∫–∏ –¥–ª—è topology]
6. **[NetworkX](https://networkx.org/documentation/stable/tutorial.html)** used for construction of graphs

## Data preparation

1) Books come in different formats, but we decided to use .txt. We wrote a converter from .fb2 ‚Üí .txt, which can be found here: [fb2_txt_converter.py](code/fb2_txt_converter.py)

2) We tested several preprocessing methods (see [–Ω–∞–∑–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–∞]):

- Original text
- Lemmatized text (e.g., –±–µ–≥–∞–ª ‚Üí –±–µ–≥–∞—Ç—å, —Å–ª–æ–Ω—É ‚Üí —Å–ª–æ–Ω)
- Lemmatized text without punctuation and stopwords (stopwords ‚Äî the most common words in a language).
- Reduced to parts of speech (e.g., —Å–ª–æ–Ω ‚Üí NOUN, –±–µ–≥–∞—Ç—å ‚Üí VERB) without stopwords/punctuation
- Parts of speech encoded as one-letter codes:
  - a = ADJ   (adjective)
  - b = ADP   (adposition)
  - c = ADV   (adverb)
  - etc...

## Generation of Neuroversion of our author books



## Analysis

1) We used the stylo R package ‚Äî a standard tool in stylometry for comparing texts using word frequencies.

–ê–Ω–∞–ª–∏–∑ –≤ –±–∏–±–ª–∏–æ—Ç–µ–∫–µ Stylo –æ—Å–Ω–æ–≤–∞–Ω –Ω–∞ –¥–µ–ª—å—Ç–µ –ë–µ—Ä—Ä–æ—É–∑–∞. –≠—Ç–∞ –º–µ—Ç—Ä–∏–∫–∞ –ø–æ–∑–≤–æ–ª—è–µ—Ç –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –Ω–∞—Å–∫–æ–ª—å–∫–æ —Ç–µ–∫—Å—Ç—ã –ø–æ—Ö–æ–∂–∏ –∏–ª–∏ –æ—Ç–ª–∏—á–∞—é—Ç—Å—è –¥—Ä—É–≥ –æ—Ç –¥—Ä—É–≥–∞. –í –ø–µ—Ä–≤—É—é –æ—á–µ—Ä–µ–¥—å –ø–æ –≤—Å–µ–º—É –∫–æ—Ä–ø—É—Å—É —Ä–∞—Å—Å—á–∏—Ç—ã–≤–∞—é—Ç—Å—è —á–∞—Å—Ç–æ—Ç—ã –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–Ω–Ω–æ–≥–æ —á–∏—Å–ª–∞ —Å–ª–æ–≤. –ù–∞ –æ—Å–Ω–æ–≤–µ –ø–æ–ª—É—á–µ–Ω–Ω—ã—Ö —á–∞—Å—Ç–æ—Ç —Å—Ç—Ä–æ–∏—Ç—Å—è –º–∞—Ç—Ä–∏—Ü–∞ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–π –¥–ª—è –∫–∞–∂–¥–æ–π –ø–∞—Ä—ã –∫–Ω–∏–≥ –∏–∑ –∫–æ—Ä–ø—É—Å–∞.

#### –ê–≤—Ç–æ—Ä –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –¥—Ä—É–≥–∏—Ö –∞–≤—Ç–æ—Ä–æ–≤ –¥–µ—Ç–µ–∫—Ç–∏–≤–æ–≤

–ú—ã –æ–ø—Ä–µ–¥–µ–ª–∏–ª–∏, —á—Ç–æ –Ω–∞—à –≤—ã–±—Ä–∞–Ω–Ω—ã–π –∞–≤—Ç–æ—Ä –æ—Ç–¥–µ–ª—è–µ—Ç—Å—è –æ—Ç –¥—Ä—É–≥–∏—Ö –ø–∏—Å–∞—Ç–µ–ª–µ–π, —Ä–∞–±–æ—Ç–∞—é—â–∏—Ö –≤ —ç—Ç–æ–º —Å—Ç–∏–ª–µ, –∑–Ω–∞—á–∏—Ç –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏ —Å—Ç–∏–ª—è —ç—Ç–æ–≥–æ –∞–≤—Ç–æ—Ä–∞ –Ω–µ –æ–ø—Ä–µ–¥–µ–ª—è—é—Ç—Å—è —Ç–æ–ª—å–∫–æ –ø—Ä–∏–Ω–∞–¥–∂–ª–µ–∂–Ω–æ—Å—Ç—å—é –∫ –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–Ω–æ–º—É –∂–∞–Ω—Ä—É.

<img src="images/author_in_detective_field.png" alt="Detective tree" width="600">

#### –ò–∑–º–µ–Ω–µ–Ω–∏–µ —Å—Ç–∏–ª—è –∞–≤—Ç–æ—Ä–∞ —Å —Ç–µ—á–µ–Ω–∏–µ–º –≤—Ä–µ–º–µ–Ω–∏

–ë–ª–∞–≥–æ–¥–∞—Ä—è –º–µ—Ç—Ä–∏–∫–∞–º —Å—Ç–∏–ª–æ–º–µ—Ç—Ä–∏–∏ –º–æ–∂–Ω–æ –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –∏–∑–º–µ–Ω–∏–ª—Å—è –ª–∏ —Å—Ç–∏–ª—å –∞–≤—Ç–æ—Ä–∞ —Ç–µ–∫—Å—Ç–∞ —Å —Ç–µ—á–µ–Ω–∏–µ–º –≤—Ä–µ–º–µ–Ω–∏. –ù–∞—à –∑–∞–≥–∞–¥–æ—á–Ω—ã–π –∞–≤—Ç–æ—Ä –æ—á–µ–Ω—å –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω—ã–π –∏ –ø–∏—à–µ—Ç –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ –±–æ–ª—å—à–µ 20 –ª–µ—Ç. –ú–æ–∂–Ω–æ –ª–∏ –∑–∞–º–µ—Ç–∏—Ç—å —Ä–∞–∑–Ω–∏—Ü—É –≤ —Å—Ç–∏–ª–µ (–∞ –º–æ–∂–µ—Ç –∏ –∞–≤—Ç–æ—Ä—Å—Ç–≤–µ) –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–π?

<img src="images/author_in_time.jpg" alt="Author's style" width="600">

<img src="images/author_in_time_graph.png" alt="Author's style graph" width="600">

–î–∞! –ú–æ–∂–Ω–æ –æ–¥–Ω–æ–∑–Ω–∞—á–Ω–æ —Å–∫–∞–∑–∞—Ç—å, —á—Ç–æ —Ä–∞–Ω–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏—è –∞–≤—Ç–æ—Ä–∞ —Å–ª–∞–±–æ –ø–æ—Ö–æ–∂–∏ –Ω–∞ –±–æ–ª–µ–µ –ø–æ–∑–¥–Ω–∏–µ –∏ —Å–æ–≤—Å–µ–º –Ω–µ –ø–æ—Ö–æ–∂–∏–µ –Ω–∞ —Å–∞–º—ã–µ –ø–æ—Å–ª–µ–¥–Ω–∏–µ. –ú—ã –Ω–µ –≥–æ—Ç–æ–≤—ã —Å–¥–µ–ª–∞—Ç—å –æ–¥–Ω–æ–∑–Ω–∞—á–Ω—ã–π –≤—ã–≤–æ–¥ –æ –ø—Ä–∏—á–∏–Ω–∞—Ö –Ω–∞–±–ª—é–¥–∞–µ–º—ã—Ö –∑–∞–∫–æ–Ω–æ–º–µ—Ä–Ω–æ—Å—Ç–µ–π, –Ω–æ –ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º, —á—Ç–æ —ç—Ç–æ —Å–≤—è–∑–∞–Ω–æ –ª–∏–±–æ —Å –±–æ–ª—å—à–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º —Ä–µ–¥–∞–∫—Ç—É—Ä—ã, –ª–∏–±–æ —Å –ø–ª–∞–≥–∏–∞—Ç–æ–º, –∞ –≤–æ–∑–º–æ–∂–Ω–æ —Å –Ω–∞–ø–∏—Å–∞–Ω–∏–µ–º –∫–Ω–∏–≥–∏ —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ –Ω–∞–Ω—è—Ç—ã–º —á–µ–ª–æ–≤–µ–∫–æ–º.

[example of the frequency table]

2) Based on word frequency tables from stylo, we applied dimentionality reduction:

- PCA
- tSNE

[–∫–∞—Ä—Ç–∏–Ω–∫–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∏ –æ–ø–∏—Å–∞–Ω–∏–µ]

3) Topological analysis

The idea:

1. Get frequency of each word/POS (part of speech) in each book.
2. Compute pairwise distances (cosine distance).
3. Plot distances as a topological graph and tune parameters to detect clusters.

Details for math fans: [—Å—Å—ã–ª–∫–∞ –Ω–∞ —Å—Ç–∞—Ç—å—é]

[–∫–∞—Ä—Ç–∏–Ω–∫–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∏ –æ–ø–∏—Å–∞–Ω–∏–µ]

4) Adaptation of k-mer analysis from bioinformatics

We wanted to analyze how much an author maintains its grammatical style, how it changes and evolves throughout the works. So we adapted k-mer frequency analysis to grammar patterns. For k from 2 to 10, we calculated frequency of POS sequences for each book. By comparing sets between books and with the general pool of k-dimensional sets, we can check how similar each text is to the next. Then we cluster the results.

[–∫–∞—Ä—Ç–∏–Ω–∫–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∏ –æ–ø–∏—Å–∞–Ω–∏–µ]
